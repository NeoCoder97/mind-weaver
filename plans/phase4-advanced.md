# é˜¶æ®µ 4 - é«˜çº§åŠŸèƒ½ä¸ç”Ÿæ€æ•´åˆ

## é˜¶æ®µç›®æ ‡

æ‰“é€ å®Œæ•´çš„æƒ…æŠ¥åˆ†æå¹³å°ï¼ŒåŒ…æ‹¬å¤šæºé‡‡é›†ã€äº‹ä»¶èšç±»ã€è¶‹åŠ¿åˆ†æå’Œæ™ºèƒ½æ´å¯Ÿã€‚

---

## æ ¸å¿ƒåŠŸèƒ½

### 1. å¤šæºé‡‡é›†æ‰©å±•
- ç¤¾äº¤åª’ä½“ API é›†æˆï¼ˆTwitter/X, Reddit, Hacker Newsï¼‰
- ç½‘é¡µç›‘æ§ä¸å˜æ›´æ£€æµ‹
- é‚®ä»¶åˆ—è¡¨è®¢é˜…
- æ’­å®¢/è§†é¢‘è®¢é˜…æº
- å­¦æœ¯è®ºæ–‡è·Ÿè¸ªï¼ˆarXiv, Google Scholarï¼‰

### 2. äº‹ä»¶æ£€æµ‹ä¸èšç±»
- ç›¸ä¼¼æ–‡ç« èšåˆ
- äº‹ä»¶æ¼”åŒ–è¿½è¸ª
- çƒ­ç‚¹äº‹ä»¶å‘ç°
- äº‹ä»¶æ‘˜è¦ç”Ÿæˆ

### 3. è¶‹åŠ¿åˆ†æ
- å…³é”®è¯è¶‹åŠ¿ï¼ˆæ—¶é—´åºåˆ—ï¼‰
- ä¸»é¢˜çƒ­åº¦å˜åŒ–
- çªå‘äº‹ä»¶æ£€æµ‹
- é¢„æµ‹æ€§åˆ†æ

### 4. æ™ºèƒ½æ´å¯Ÿ
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
- å…³è”åˆ†æ
- å¼‚å¸¸æ£€æµ‹
- çŸ¥è¯†å›¾è°±

---

## æŠ€æœ¯é€‰å‹

| åŠŸèƒ½ | æŠ€æœ¯æ–¹æ¡ˆ |
|------|----------|
| å¤šæºé‡‡é›† | å„å¹³å° API + é€šç”¨çˆ¬è™« |
| äº‹ä»¶èšç±» | HDBSCAN / DBSCAN |
| è¶‹åŠ¿åˆ†æ | Prophet / statsmodels |
| æ—¶é—´åºåˆ— | pandas + matplotlib/plotly |
| çŸ¥è¯†å›¾è°± | NetworkX / Neo4j |
| æŠ¥å‘Šç”Ÿæˆ | Jinja2 + WeasyPrint |
| æ•°æ®å­˜å‚¨ | PostgreSQL / TimescaleDB |

---

## ä»»åŠ¡æ‹†è§£

### PHASE 4.1: å¤šæºé‡‡é›†æ¶æ„ (6-8h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.1.1 | è®¾è®¡ç»Ÿä¸€é‡‡é›†æ¥å£ | æŠ½è±¡æ•°æ®æº |
| 4.1.2 | å®ç° Twitter/X é‡‡é›† | API é›†æˆ |
| 4.1.3 | å®ç° Reddit é‡‡é›† | API + RSS |
| 4.1.4 | å®ç° Hacker News é‡‡é›† | API |
| 4.1.5 | å®ç°ç½‘é¡µç›‘æ§ | å˜æ›´æ£€æµ‹ |
| 4.1.6 | å®ç° arXiv é‡‡é›† | å­¦æœ¯è®ºæ–‡ |
| 4.1.7 | å®ç°é‚®ä»¶åˆ—è¡¨è§£æ | mbox/IMAP |
| 4.1.8 | é‡‡é›†å™¨æ’ä»¶ç³»ç»Ÿ | å¯æ‰©å±•æ¶æ„ |

**ç»Ÿä¸€æ•°æ®æºæ¥å£ï¼š**
```python
class DataSource(ABC):
    @abstractmethod
    async def fetch(self) -> List[RawItem]:
        pass

    @abstractmethod
    def get_metadata(self) -> SourceMetadata:
        pass

    @abstractmethod
    def validate_config(self) -> bool:
        pass
```

### PHASE 4.2: äº‹ä»¶æ£€æµ‹ä¸èšç±» (6-7h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.2.1 | å®ç°æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®— | è¯­ä¹‰ç›¸ä¼¼åº¦ |
| 4.2.2 | å®ç° HDBSCAN èšç±» | äº‹ä»¶èšç±» |
| 4.2.3 | å®ç°äº‹ä»¶æ¼”åŒ–è¿½è¸ª | æ—¶é—´çª—å£èšåˆ |
| 4.2.4 | å®ç°çƒ­ç‚¹å‘ç°ç®—æ³• | çªå‘æ£€æµ‹ |
| 4.2.5 | äº‹ä»¶æ‘˜è¦ç”Ÿæˆ | å¤šæ–‡æ¡£æ‘˜è¦ |
| 4.2.6 | äº‹ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç† | åˆ›å»ºã€æ›´æ–°ã€å½’æ¡£ |

**äº‹ä»¶æ¨¡å‹ï¼š**
```python
class Event:
    - id, title, summary
    - cluster_id: String       # èšç±»æ ‡è¯†
    - entry_ids: JSON          # åŒ…å«çš„æ¡ç›®
    - first_seen_at, last_seen_at
    - article_count: Integer   # æ–‡ç« æ•°é‡
    - trend_score: Float       # çƒ­åº¦åˆ†æ•°
    - keywords: JSON
    - entities: JSON           # å‘½åå®ä½“
    - status: emerging | active | fading | archived
```

### PHASE 4.3: è¶‹åŠ¿åˆ†æç³»ç»Ÿ (5-6h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.3.1 | å®ç°å…³é”®è¯è¶‹åŠ¿åˆ†æ | æ—¶é—´åºåˆ—èšåˆ |
| 4.3.2 | å®ç°ä¸»é¢˜çƒ­åº¦è¿½è¸ª | æ»‘åŠ¨çª—å£ç»Ÿè®¡ |
| 4.3.3 | å®ç°çªå‘äº‹ä»¶æ£€æµ‹ | å¼‚å¸¸æ£€æµ‹ç®—æ³• |
| 4.3.4 | å®ç°è¶‹åŠ¿é¢„æµ‹ | Prophet/ARIMA |
| 4.3.5 | å¯è§†åŒ– API | å›¾è¡¨æ•°æ® |
| 4.3.6 | è¶‹åŠ¿æŠ¥å‘Šç”Ÿæˆ | å®šæœŸæŠ¥å‘Š |

**è¶‹åŠ¿æ•°æ®æ¨¡å‹ï¼š**
```python
class KeywordTrend:
    - keyword: String
    - counts: JSON            # æ—¶é—´åºåˆ—è®¡æ•°
    - scores: JSON            # çƒ­åº¦åˆ†æ•°
    - velocity: Float         # å¢é•¿é€Ÿåº¦
    - acceleration: Float     # åŠ é€Ÿåº¦
    - predicted: JSON         # é¢„æµ‹å€¼

class TopicTrend:
    - topic_id: Integer
    - topic_name: String
    - entry_volume: JSON      # æ–‡ç« é‡è¶‹åŠ¿
    - engagement_score: JSON  # å‚ä¸åº¦è¶‹åŠ¿
    - sentiment_score: JSON   # æƒ…æ„Ÿè¶‹åŠ¿
```

### PHASE 4.4: çŸ¥è¯†å›¾è°±æ„å»º (4-5h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.4.1 | å®ä½“è¯†åˆ«ä¸æŠ½å– | NER æ¨¡å‹ |
| 4.4.2 | å®ç°å…³ç³»æŠ½å– | å…±ç°åˆ†æ |
| 4.4.3 | æ„å»ºå›¾æ•°æ®åº“ | NetworkX/Neo4j |
| 4.4.4 | å®ç°å›¾æŸ¥è¯¢ API | è·¯å¾„æŸ¥æ‰¾ã€ç¤¾åŒºå‘ç° |
| 4.4.5 | çŸ¥è¯†å›¾è°±å¯è§†åŒ– | äº¤äº’å¼å±•ç¤º |

**çŸ¥è¯†å›¾è°±æ¨¡å‹ï¼š**
```python
class Entity:
    - id, name, type          # äººç‰©/ç»„ç»‡/åœ°ç‚¹/æ¦‚å¿µ
    - aliases: JSON
    - description: Text
    - metadata: JSON

class Relation:
    - id, source_id, target_id
    - relation_type: mentions | associated_with | employs | ...
    - weight: Float           # å…³ç³»å¼ºåº¦
    - context: Text
```

### PHASE 4.5: è‡ªåŠ¨åŒ–æŠ¥å‘Šç³»ç»Ÿ (4-5h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.5.1 | è®¾è®¡æŠ¥å‘Šæ¨¡æ¿ç³»ç»Ÿ | å¯è‡ªå®šä¹‰ |
| 4.5.2 | å®ç°æ—¥æŠ¥ç”Ÿæˆå™¨ | æ¯æ—¥æ‘˜è¦ |
| 4.5.3 | å®ç°å‘¨æŠ¥ç”Ÿæˆå™¨ | è¶‹åŠ¿åˆ†æ |
| 4.5.4 | å®ç° PDF å¯¼å‡º | WeasyPrint |
| 4.5.5 | å®ç°é‚®ä»¶æ¨é€ | SMTP |
| 4.5.6 | æŠ¥å‘Šè°ƒåº¦ç³»ç»Ÿ | å®šæ—¶ç”Ÿæˆ |

**æŠ¥å‘Šæ¨¡æ¿ï¼š**
```markdown
# æƒ…æŠ¥æ—¥æŠ¥ - {date}

## çƒ­ç‚¹äº‹ä»¶
{top_events}

## å…³é”®è¯è¶‹åŠ¿
{keyword_trends}

## æ–°å¢è®¢é˜…æº
{new_feeds}

## ç»Ÿè®¡æ¦‚è§ˆ
- æ–°å¢æ¡ç›®: {entry_count}
- æ´»è·ƒäº‹ä»¶: {active_events}
```

### PHASE 4.6: é«˜çº§æœç´¢ä¸è¿‡æ»¤ (3-4h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.6.1 | å®ç°å…¨æ–‡æœç´¢ | å‘é‡æœç´¢ |
| 4.6.2 | å®ç°è¯­ä¹‰æœç´¢ | è¯­ä¹‰ç›¸ä¼¼åº¦ |
| 4.6.3 | å®ç°ç»„åˆè¿‡æ»¤å™¨ | å¤æ‚æŸ¥è¯¢ |
| 4.6.4 | æœç´¢ç»“æœæ’åº | ç›¸å…³æ€§æ’åº |
| 4.6.5 | ä¿å­˜æœç´¢æŸ¥è¯¢ | å¿«é€Ÿè®¿é—® |

### PHASE 4.7: å®æ—¶é€šçŸ¥ç³»ç»Ÿ (3-4h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.7.1 | å®ç°é€šçŸ¥è§„åˆ™å¼•æ“ | å¯é…ç½®è§¦å‘æ¡ä»¶ |
| 4.7.2 | æ”¯æŒå¤šæ¸ é“é€šçŸ¥ | é‚®ä»¶/Webhook/Telegram |
| 4.7.3 | å®ç°é€šçŸ¥å»é‡ | é¿å…è½°ç‚¸ |
| 4.7.4 | é€šçŸ¥å†å²è®°å½• | æŸ¥çœ‹å’Œè¿½æº¯ |

### PHASE 4.8: æ•°æ®å¯è§†åŒ–ä»ªè¡¨æ¿ (5-6h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.8.1 | è®¾è®¡ä»ªè¡¨æ¿å¸ƒå±€ | å¤šæ¨¡å—å±•ç¤º |
| 4.8.2 | å®ç°è¶‹åŠ¿å›¾è¡¨ | æŠ˜çº¿å›¾ã€é¢ç§¯å›¾ |
| 4.8.3 | å®ç°äº‹ä»¶æ—¶é—´çº¿ | å¯è§†åŒ–äº‹ä»¶æµ |
| 4.8.4 | å®ç°ç½‘ç»œå›¾è°± | å…³ç³»å¯è§†åŒ– |
| 4.8.5 | å®ç°è¯äº‘å±•ç¤º | å…³é”®è¯å¯è§†åŒ– |
| 4.8.6 | å®æ—¶æ•°æ®æ›´æ–° | WebSocket |

### PHASE 4.9: API ä¸é›†æˆ (3-4h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.9.1 | Webhook ç³»ç»Ÿ | å¤–éƒ¨é›†æˆ |
| 4.9.2 | GraphQL API | çµæ´»æŸ¥è¯¢ |
| 4.9.3 | å¯¼å…¥/å¯¼å‡ºåŠŸèƒ½ | æ•°æ®è¿ç§» |
| 4.9.4 | API å¯†é’¥ç®¡ç† | å®‰å…¨è®¿é—® |

### PHASE 4.10: æµ‹è¯•ä¸ä¼˜åŒ– (4-5h)

| ID | ä»»åŠ¡ | éªŒæ”¶æ ‡å‡† |
|----|------|----------|
| 4.10.1 | å¤šæºé‡‡é›†æµ‹è¯• | éªŒè¯å„æ•°æ®æº |
| 4.10.2 | äº‹ä»¶èšç±»è¯„ä¼° | è´¨é‡æŒ‡æ ‡ |
| 4.10.3 | è¶‹åŠ¿åˆ†æéªŒè¯ | å‡†ç¡®æ€§æµ‹è¯• |
| 4.10.4 | æ€§èƒ½å‹åŠ›æµ‹è¯• | å¹¶å‘å¤„ç† |
| 4.10.5 | æ•°æ®å®‰å…¨å®¡è®¡ | æ•æ„Ÿä¿¡æ¯ä¿æŠ¤ |

---

## éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½æ€§
- âœ… æ”¯æŒè‡³å°‘ 5 ç§æ•°æ®æº
- âœ… äº‹ä»¶èšç±»å‡†ç¡®ç‡ â‰¥ 70%
- âœ… è¶‹åŠ¿é¢„æµ‹è¯¯å·® â‰¤ 30%
- âœ… æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ

### æ€§èƒ½
- âœ… å¤šæºé‡‡é›†å»¶è¿Ÿ â‰¤ 30 ç§’
- âœ… äº‹ä»¶èšç±»æ—¶é—´ â‰¤ 5 åˆ†é’Ÿ
- âœ… ä»ªè¡¨æ¿åŠ è½½ â‰¤ 3 ç§’
- âœ… æ”¯æŒ 10000+ æ¡ç›®å¤„ç†

### æ•ˆæœ
- âœ… çƒ­ç‚¹å‘ç°å‡†ç¡®ç‡ â‰¥ 75%
- âœ… ç”¨æˆ·ç•™å­˜ç‡æå‡

### ä»£ç è´¨é‡
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 70%
- âœ… æ–‡æ¡£å®Œæ•´
- âœ… å¯æ‰©å±•æ¶æ„

---

## æ—¶é—´ä¼°ç®—

| é˜¶æ®µ | é¢„è®¡æ—¶é—´ |
|------|----------|
| 4.1 å¤šæºé‡‡é›† | 6-8h |
| 4.2 äº‹ä»¶èšç±» | 6-7h |
| 4.3 è¶‹åŠ¿åˆ†æ | 5-6h |
| 4.4 çŸ¥è¯†å›¾è°± | 4-5h |
| 4.5 æŠ¥å‘Šç³»ç»Ÿ | 4-5h |
| 4.6 é«˜çº§æœç´¢ | 3-4h |
| 4.7 é€šçŸ¥ç³»ç»Ÿ | 3-4h |
| 4.8 æ•°æ®å¯è§†åŒ– | 5-6h |
| 4.9 API é›†æˆ | 3-4h |
| 4.10 æµ‹è¯•ä¼˜åŒ– | 4-5h |
| **æ€»è®¡** | **43-58h** |

**å»ºè®®å¼€å‘å‘¨æœŸ**: æŒ‰æ¯å¤© 2-3 å°æ—¶å¼€å‘ï¼Œé¢„è®¡ **4-6 å‘¨** å®Œæˆ

---

## å…³é”®ä¾èµ–

```toml
[project.dependencies]
"tweepy>=4.14.0",            # Twitter API
"praw>=7.7.0",               # Reddit API
"requests>=2.31.0",          # HTTP è¯·æ±‚
"beautifulsoup4>=4.12.0",    # HTML è§£æ
"hdbscan>=0.8.33",           # èšç±»ç®—æ³•
"scikit-learn>=1.3.0",       # æœºå™¨å­¦ä¹ 
"prophet>=1.1.4",            # æ—¶é—´åºåˆ—é¢„æµ‹
"networkx>=3.2.0",           # å›¾è®¡ç®—
"plotly>=5.18.0",            # äº¤äº’å¼å›¾è¡¨
"weasyprint>=60.0",          # PDF ç”Ÿæˆ
"spacy>=3.7.0",              # NLP + NER
"graphql-core>=3.2.5",       # GraphQL
"arrow>=1.3.0",              # æ—¶åŒºå¤„ç†
```

---

## äº‹ä»¶æ£€æµ‹ç®—æ³•

### ç›¸ä¼¼åº¦è®¡ç®—
```python
# ä½¿ç”¨æ–‡æœ¬åµŒå…¥è®¡ç®—ç›¸ä¼¼åº¦
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_similarity(entry1, entry2):
    emb1 = model.encode(entry1.title + " " + entry1.summary)
    emb2 = model.encode(entry2.title + " " + entry2.summary)
    return cosine_similarity(emb1, emb2)
```

### èšç±»ç®—æ³•
```python
import hdbscan
import numpy as np

# è·å–æ‰€æœ‰æ¡ç›®çš„åµŒå…¥
embeddings = [model.encode(e.title) for e in entries]

# HDBSCAN èšç±»
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=3,
    min_samples=1,
    metric='cosine'
)
labels = clusterer.fit_predict(embeddings)

# åˆ›å»ºäº‹ä»¶ç»„
events = {}
for entry, label in zip(entries, labels):
    if label not in events:
        events[label] = []
    events[label].append(entry)
```

### çƒ­ç‚¹å‘ç°
```python
def detect_hot_events(events, window=24h):
    hot_events = []
    for event in events:
        # è®¡ç®—æ—¶é—´çª—å£å†…çš„æ–‡ç« å¢é•¿ç‡
        velocity = calculate_growth_rate(event, window)
        # è®¡ç®—å‚ä¸åº¦
        engagement = calculate_engagement(event)
        # çƒ­åº¦åˆ†æ•°
        hot_score = 0.7 * velocity + 0.3 * engagement
        if hot_score > threshold:
            hot_events.append(event)
    return sort_by_score(hot_events)
```

---

## è¶‹åŠ¿åˆ†æç¤ºä¾‹

### å…³é”®è¯è¶‹åŠ¿
```python
# æ—¶é—´çª—å£èšåˆ
def analyze_keyword_trend(keyword, days=30):
    daily_counts = []
    for day in range(days):
        count = count_keyword_occurrences(keyword, day)
        daily_counts.append(count)

    # è®¡ç®—å¢é•¿é€Ÿåº¦
    velocity = calculate_velocity(daily_counts)

    # é¢„æµ‹æœªæ¥è¶‹åŠ¿
    predicted = predict_trend(daily_counts)

    return {
        'historical': daily_counts,
        'velocity': velocity,
        'predicted': predicted
    }
```

### çªå‘æ£€æµ‹
```python
def detect_bursts(keyword_series):
    # ä½¿ç”¨ z-score æ£€æµ‹å¼‚å¸¸
    mean = np.mean(keyword_series)
    std = np.std(keyword_series)

    bursts = []
    for i, value in enumerate(keyword_series):
        z_score = (value - mean) / std
        if z_score > 3:  # 3-sigma è§„åˆ™
            bursts.append({
                'timestamp': i,
                'value': value,
                'z_score': z_score
            })
    return bursts
```

---

## æŠ¥å‘Šç”Ÿæˆæµç¨‹

```python
class ReportGenerator:
    def generate_daily_report(self, date):
        # æ”¶é›†æ•°æ®
        top_events = get_top_events(date, n=5)
        trending_keywords = get_trending_keywords(date, n=10)
        new_entries = get_new_entries(date)

        # æ¸²æŸ“æ¨¡æ¿
        html = render_template('daily_report.html', {
            'date': date,
            'events': top_events,
            'keywords': trending_keywords,
            'stats': {
                'entry_count': len(new_entries),
                'feed_count': count_active_feeds()
            }
        })

        # å¯¼å‡º PDF
        pdf = convert_html_to_pdf(html)

        return pdf
```

---

## ç›®å½•ç»“æ„æ‰©å±•

```
spider-aggregation/
â”œâ”€â”€ src/spider_aggregation/
â”‚   â”œâ”€â”€ sources/                # ğŸ†• å¤šæºé‡‡é›†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # æ•°æ®æºåŸºç±»
â”‚   â”‚   â”œâ”€â”€ twitter_source.py
â”‚   â”‚   â”œâ”€â”€ reddit_source.py
â”‚   â”‚   â”œâ”€â”€ hn_source.py
â”‚   â”‚   â”œâ”€â”€ webpage_monitor.py
â”‚   â”‚   â”œâ”€â”€ arxiv_source.py
â”‚   â”‚   â””â”€â”€ email_source.py
â”‚   â”‚
â”‚   â”œâ”€â”€ clustering/             # ğŸ†• äº‹ä»¶èšç±»
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ event_detector.py  # äº‹ä»¶æ£€æµ‹
â”‚   â”‚   â”œâ”€â”€ clusterer.py       # èšç±»ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ event_tracker.py   # äº‹ä»¶è¿½è¸ª
â”‚   â”‚   â””â”€â”€ event_summarizer.py # äº‹ä»¶æ‘˜è¦
â”‚   â”‚
â”‚   â”œâ”€â”€ trends/                 # ğŸ†• è¶‹åŠ¿åˆ†æ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ analyzer.py        # è¶‹åŠ¿åˆ†æ
â”‚   â”‚   â”œâ”€â”€ forecaster.py      # é¢„æµ‹
â”‚   â”‚   â”œâ”€â”€ burst_detector.py  # çªå‘æ£€æµ‹
â”‚   â”‚   â”” visualizer.py        # å¯è§†åŒ–
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge/              # ğŸ†• çŸ¥è¯†å›¾è°±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py # å®ä½“æŠ½å–
â”‚   â”‚   â”œâ”€â”€ relation_extractor.py # å…³ç³»æŠ½å–
â”‚   â”‚   â”œâ”€â”€ graph_builder.py   # å›¾æ„å»º
â”‚   â”‚   â””â”€â”€ graph_query.py     # å›¾æŸ¥è¯¢
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                # ğŸ†• æŠ¥å‘Šç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py       # æŠ¥å‘Šç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ templates/         # æŠ¥å‘Šæ¨¡æ¿
â”‚   â”‚   â”‚   â”œâ”€â”€ daily.html
â”‚   â”‚   â”‚   â”œâ”€â”€ weekly.html
â”‚   â”‚   â”‚   â””â”€â”€ monthly.html
â”‚   â”‚   â””â”€â”€ scheduler.py       # æŠ¥å‘Šè°ƒåº¦
â”‚   â”‚
â”‚   â”œâ”€â”€ notifications/          # ğŸ†• é€šçŸ¥ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py          # é€šçŸ¥å¼•æ“
â”‚   â”‚   â”œâ”€â”€ channels/          # é€šçŸ¥æ¸ é“
â”‚   â”‚   â”‚   â”œâ”€â”€ email.py
â”‚   â”‚   â”‚   â”œâ”€â”€ webhook.py
â”‚   â”‚   â”‚   â””â”€â”€ telegram.py
â”‚   â”‚   â””â”€â”€ rules.py           # é€šçŸ¥è§„åˆ™
â”‚   â”‚
â”‚   â””â”€â”€ search/                 # ğŸ†• é«˜çº§æœç´¢
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ vector_search.py   # å‘é‡æœç´¢
â”‚       â”œâ”€â”€ semantic_search.py # è¯­ä¹‰æœç´¢
â”‚       â””â”€â”€ query_builder.py   # æŸ¥è¯¢æ„å»º
â”‚
â”œâ”€â”€ web/src/views/
â”‚   â”œâ”€â”€ Dashboard.vue          # ğŸ†• ä»ªè¡¨æ¿
â”‚   â”œâ”€â”€ Events.vue             # ğŸ†• äº‹ä»¶è§†å›¾
â”‚   â”œâ”€â”€ Trends.vue             # ğŸ†• è¶‹åŠ¿è§†å›¾
â”‚   â”œâ”€â”€ KnowledgeGraph.vue     # ğŸ†• çŸ¥è¯†å›¾è°±
â”‚   â””â”€â”€ Reports.vue            # ğŸ†• æŠ¥å‘Šåˆ—è¡¨
â”‚
â”œâ”€â”€ models/                     # è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ hdbscan_model.pkl
â”‚   â”œâ”€â”€ sentence_transformer/
â”‚   â””â”€â”€ ner_model/
â”‚
â””â”€â”€ exports/                    # å¯¼å‡ºæ–‡ä»¶
    â””â”€â”€ reports/
```

---

## æ•°æ®å¯è§†åŒ–ç¤ºä¾‹

### è¶‹åŠ¿å›¾è¡¨
```python
import plotly.graph_objects as go

def create_trend_chart(keyword_data):
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=keyword_data['dates'],
        y=keyword_data['counts'],
        mode='lines',
        name='å®é™…å€¼'
    ))
    fig.add_trace(go.Scatter(
        x=keyword_data['dates'],
        y=keyword_data['predicted'],
        mode='lines',
        name='é¢„æµ‹å€¼',
        line=dict(dash='dash')
    ))
    return fig.to_html()
```

### äº‹ä»¶æ—¶é—´çº¿
```python
def create_event_timeline(events):
    fig = go.Figure()
    for event in events:
        fig.add_trace(go.Scatter(
            x=[event.start, event.end],
            y=[event.id, event.id],
            mode='lines+markers',
            name=event.title
        ))
    return fig.to_html()
```

---

## API æ‰©å±•ç¤ºä¾‹

```python
# äº‹ä»¶ç›¸å…³
GET  /api/events                 # åˆ—å‡ºäº‹ä»¶
GET  /api/events/{id}           # äº‹ä»¶è¯¦æƒ…
GET  /api/events/{id}/timeline  # äº‹ä»¶æ—¶é—´çº¿
GET  /api/events/trending       # çƒ­ç‚¹äº‹ä»¶

# è¶‹åŠ¿ç›¸å…³
GET  /api/trends/keywords       # å…³é”®è¯è¶‹åŠ¿
GET  /api/trends/topics         # ä¸»é¢˜è¶‹åŠ¿
GET  /api/trends/bursts         # çªå‘äº‹ä»¶

# çŸ¥è¯†å›¾è°±
GET  /api/knowledge/entities    # å®ä½“åˆ—è¡¨
GET  /api/knowledge/graph       # å›¾æ•°æ®
GET  /api/knowledge/paths       # å®ä½“å…³ç³»è·¯å¾„

# æŠ¥å‘Š
GET  /api/reports               # æŠ¥å‘Šåˆ—è¡¨
POST /api/reports/generate      # ç”ŸæˆæŠ¥å‘Š
GET  /api/reports/{id}/download # ä¸‹è½½æŠ¥å‘Š
```
